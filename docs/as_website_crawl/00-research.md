## Возможности режима `website_crawl` для Git-репозиториев

Режим веб-краулера (`website_crawl`) в Dify предназначен для автоматического рекурсивного сбора контента со связанных страниц или файлов, что соответствует задаче импорта целого репозитория Git. Такой плагин наследуется от класса `WebsiteCrawlDatasource` и реализует метод `_get_website_crawl`, позволяющий вернуть сразу множество “страниц” (в нашем случае — файлов репозитория) за одно выполнение[docs.dify.ai](https://docs.dify.ai/en/develop-plugin/dev-guides-and-walkthroughs/datasource-plugin#:~:text=In%20the%20main%20logic%20code,completed). Это означает, что **режим `website_crawl` способен удовлетворить всем требованиям:**

- **Полная загрузка репозитория при инициализации:** В режиме `website_crawl` плагин может сразу просканировать весь репозиторий рекурсивно и вернуть содержание каждого файла. Dify поддерживает множественную выдачу результатов за одно выполнение: метод `_get_website_crawl` может формировать список `WebSiteInfoDetail` – каждый элемент представляет отдельный документ с полями `content` (текст файла), `title` (название/путь файла) и `source_url` (идентификатор или ссылка на источник)[docs.dify.ai](https://docs.dify.ai/en/develop-plugin/dev-guides-and-walkthroughs/datasource-plugin#:~:text=crawl_res.status%20%3D%20,%29)[docs.dify.ai](https://docs.dify.ai/en/develop-plugin/dev-guides-and-walkthroughs/datasource-plugin#:~:text=type%3A%20string%20description%3A%20the%20content,the%20description%20of%20the%20website). Таким образом, все файлы репозитория могут быть импортированы как отдельные документы за один запуск плагина.
    
- **Инкрементальная синхронизация изменений:** Плагин на `website_crawl` можно вызывать повторно (например, при нажатии кнопки “Sync” в базе знаний или через API Dify) для обновления данных. Предполагается, что при повторной индексации плагин получит актуальное состояние репозитория (например, с помощью `git pull` для удалённого репо или проверки локального пути) и вновь вернёт список файлов. Dify сравнивает новые данные с ранее загруженными документами: если какие-то документы ранее существовали, а теперь отсутствуют в выдаче плагина, система пометит их как **orphaned** (осиротевшие) и исключит из активной базы знаний (т.е. они считаются удалёнными из источника). Добавленные же файлы появятся как новые документы. Таким образом достигается инкрементальная синхронизация — новые файлы добавляются, а удалённые помечаются как удалённые (orphaned) без дополнительного ручного вмешательства.
    
- **Каждый файл — отдельный документ:** Структура данных, возвращаемая веб-краулер-плагином, изначально рассчитана на множество отдельных страниц. В контексте Git-репозитория плагин будет создавать объект `WebSiteInfo` с полем `web_info_list`, содержащим несколько объектов `WebSiteInfoDetail` — по одному на каждый файл[docs.dify.ai](https://docs.dify.ai/en/develop-plugin/dev-guides-and-walkthroughs/datasource-plugin#:~:text=crawl_res.status%20%3D%20,%29). Каждый такой объект включает контент файла (`content`) и метаданные (например, имя файла как `title`, путь или URL как `source_url`). Dify преобразует каждый `WebSiteInfoDetail` в отдельный документ базы знаний. Это соответствует требованию, чтобы каждый файл репозитория хранился как отдельный документ.
    
- **Входные параметры – URL/путь репозитория:** Режим `website_crawl` не ограничен работой только с веб-страницами – он способен принимать на вход любой указатель на источник данных. Плагин можно сконфигурировать так, чтобы параметром был, например, URL репозитория (HTTPS), SSH-адрес (`git@...`) или локальный путь. В манифесте провайдера указывается тип провайдера `website_crawl` и описываются необходимые параметры (например, строка с URL репо, токен доступа, путь к SSH-ключу и пр.)[docs.dify.ai](https://docs.dify.ai/en/develop-plugin/dev-guides-and-walkthroughs/datasource-plugin#:~:text=Copy). Dify позволит пользователю ввести эти данные при подключении источника. При запуске плагина ваш код сможет использовать, например, `GitPython` или вызов `git clone`, чтобы получить содержимое репозитория локально. В случае SSH-репо, плагин может требовать ключ (его можно хранить как Credential провайдера), а для приватного HTTPS-репо – токен доступа (например, Personal Access Token в качестве пароля). **Таким образом, плагин на `website_crawl` может работать с любым вариантом доступа к Git-репозиторию – по SSH, HTTPS или с локальным путем.**
    
- **Обновление по расписанию или вручную:** Dify предоставляет возможности синхронизации данных как вручную, так и (опционально) автоматически. В UI базы знаний у документов, полученных из синхронизируемых источников, как правило, есть кнопка “Sync” для запуска обновления[docs.dify.ai](https://docs.dify.ai/en/use-dify/knowledge/create-knowledge/import-text-data/sync-from-notion#:~:text=Synchronizing%20Notion%20Data). Также Dify имеет API для программной синхронизации базы знаний. Режим работы `website_crawl` совместим с этими механизмами – при вызове обновления Dify просто повторно запускает `_get_website_crawl` плагина. **Автоматическое обновление**: на текущий момент платформа Dify не выполняет плановое обновление источников по расписанию по умолчанию, но это можно реализовать внешне (через периодические API-вызовы) или с помощью **Trigger-плагина** (специального типа плагина) если требуется. Таким образом, требование об автосинхронизации “если возможно” может быть решено либо за счёт триггера, либо на уровне Dify (в будущем), но в любом случае **плагин должен поддерживать повторяемую синхронизацию без дублирования данных**.
    

**Вывод:** режим `website_crawl` полностью подходит для реализации Git-плагина с заданными требованиями. Он обеспечивает рекурсивный сбор множества документов за одно выполнение[docs.dify.ai](https://docs.dify.ai/en/develop-plugin/dev-guides-and-walkthroughs/datasource-plugin#:~:text=In%20the%20main%20logic%20code,completed), что позволяет загрузить весь репозиторий сразу, и при повторном запуске – обновить изменения, добавив новые файлы и пометив отсутствующие как удалённые.
