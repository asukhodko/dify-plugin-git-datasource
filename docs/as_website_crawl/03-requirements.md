## Требования к реализации (для разработчика)

Опираясь на вышесказанное, сформулируем конкретные требования и шаги, которые необходимо передать разработчику для переделки плагина Git-репозитория на режим `website_crawl`:

1. **Изменение типа провайдера в конфигурации:** В файле провайдера плагина (`provider/your_datasource.yaml`) нужно указать `provider_type: website_crawl` вместо текущего (например, `online_document` или `tool`). Также в `manifest.yaml` убедиться, что минимальная версия Dify >= 1.9.0 и добавлен тег `rag`[docs.dify.ai](https://docs.dify.ai/en/develop-plugin/dev-guides-and-walkthroughs/datasource-plugin#:~:text=,supported%20Dify%20version%20as%20follows)[docs.dify.ai](https://docs.dify.ai/en/develop-plugin/dev-guides-and-walkthroughs/datasource-plugin#:~:text=Copy), чтобы плагин распознавался как источник данных для базы знаний.
    
2. **Объявление класса плагина:** В коде плагина изменить родительский класс на `WebsiteCrawlDatasource`. Например:
    
    `from dify_plugin import WebsiteCrawlDatasource, WebSiteInfo, WebSiteInfoDetail class GitRepoDatasource(WebsiteCrawlDatasource):     def _get_website_crawl(self, datasource_parameters: dict[str, Any]) -> Generator:         # реализация`
    
    При этом можно удалить/переписать методы, относящиеся к старому режиму (`_get_pages`, `_get_content` или `_browse_files`), они больше не нужны.
    
3. **Параметры подключения репозитория:** Определить, какие входные параметры нужны. Минимально: **URL или путь репозитория** (строка). Дополнительно: **тип доступа/аутентификации** – например, чекбокс или выпадающий список “Authentication: None/GitHub Token/SSH Key”. Если планируется поддержка приватных репо:
    
    - Для HTTPS с токеном: нужен текстовый параметр для токена (или пара логин/пароль – но лучше токен).
        
    - Для SSH: нужен текст (многострочный) для закрытого SSH-ключа _либо_ путь к файлу ключа на сервере плагина. Надёжнее позволить вставить ключ в конфиг (Dify сохранит его зашифрованно).
        
    - Для локального пути: возможно, галочка “Local repository” и текстовое поле для пути. В этом случае не требуется авторизация.
        
    
    Эти параметры должны быть описаны в YAML (в секции `datasources:`) – они появятся в UI Dify при добавлении источника. Разработчик должен предусмотреть чтение этих параметров внутри метода `_get_website_crawl` через `datasource_parameters` аргумент.
    
4. **Логика первой синхронизации (полная загрузка):** При первом запуске (когда ещё нет локальной копии) плагин должен:
    
    - Клонировать репозиторий во временное хранилище. Можно использовать библиотеку GitPython или выполнить `git` через subprocess. Рекомендуется клонировать **без истории** (например, `git clone --depth 1`), если нас не интересуют старые коммиты – это ускорит процесс и сэкономит место. Также, если репозиторий очень большой, можно предложить опционально ограничивать ветку (по умолчанию берём `default branch`).
        
    - Если указан локальный путь – тогда клонировать не нужно, достаточно убедиться, что там `.git` директория (если путь “bare” репозиторий – можно использовать GitPython для чтения дерева).
        
    - Рекурсивно обойти все директории репозитория и собрать файлы. Каждый файл:
        
        - Прочитать как текст (utf-8, с обработкой ошибок декодирования; бинарные файлы можно пропустить или пометить).
            
        - Сформировать объект `WebSiteInfoDetail`: `title = относительный/абсолютный путь или имя файла`, `source_url = относительный путь или URL`, `content = содержимое файла`. `description` можно оставить пустой или заполнить по усмотрению (например, “File from Git repo” или краткую информацию о размере/дате – но это опционально).
            
    - Сформировать объект `WebSiteInfo`:
        
        - `status = "completed"` (если решили не стримить порциями; либо сначала выдать “processing”, а потом “completed” – см. пункт 6).
            
        - `web_info_list = [список всех WebSiteInfoDetail]`.
            
        - `total = число файлов`, `completed = число файлов` (они равны между собой, если все отданы сразу)[docs.dify.ai](https://docs.dify.ai/en/develop-plugin/dev-guides-and-walkthroughs/datasource-plugin#:~:text=crawl_res.web_info_list%20%3D%20%5B%20WebSiteInfoDetail%28%20title%3D,%29).
            
    - Вернуть результат через `yield self.create_crawl_message(crawl_res)`. После этого Dify получит все документы и начнёт их индексировать.
        
    
    > ℹ️ **Примечание:** Желательно уметь распознавать **текстовый формат** файлов. Например, `.md`, `.py`, `.txt` – читать как текст; бинарные (`.png`, `.pdf`, `.docx`) – можно не включать. Dify RAG работает лучше с текстом. Если есть необходимость включать PDF, можно дополнительно встроить их извлечение текста, но это выходит за рамки плагина (можно оставить на будущее).
    
5. **Логика последующих синхронизаций:** При повторном вызове `_get_website_crawl` плагин должен:
    
    - Найти локальную копию репозитория (например, сохранить путь или использовать кеширование в классе). Здесь нужно решить, **где хранить репозиторий между запусками**. Варианты:
        
        - Сохранить во временную папку (например, внутри контейнера плагина). Плагин-демон Dify, как правило, живёт постоянно, поэтому локальный клон можно держать между вызовами (например, сохранить путь в `self` или определённой дериктории). Но стоит учитывать, что при перезапуске контейнера всё пропадёт. В принципе, при пропаже – плагин просто снова клонирует на следующий вызов.
            
        - Либо, если плагин запускается каждый раз в чистой среде, тогда придётся каждый раз клонировать заново. Это менее эффективно, но более надёжно с точки зрения чистоты. **Рекомендуется** всё же хранить клон между синками, чтобы делать только `git fetch`/`pull`.
            
    - Выполнить `git fetch` и `git pull` для обновления до последнего коммита (на нужной ветке). Обработать возможные конфликты (в идеале, так как мы не вносим локальных изменений, `git pull` пройдёт без конфликтов).
        
    - Далее аналогично первому запуску: пройтись по файлам и сформировать список. Здесь можно также реализовать оптимизацию: определять, какие файлы новые/изменённые, чтобы, например, не перечитывать **все** файлы, а только обновлённые. Однако Dify всё равно перекусит embedding для каждого изменённого документа, так что можно упростить – перечитать все файлы (для репозиториев среднего размера это нормально).
        
    - Вернуть результат аналогично: полный список с обновлёнными данными. Как упоминалось, Dify самостоятельно распознает удалённые (те, что были в прошлый раз, но не пришли сейчас) и пометит их как удалённые документы. Новые получат новые IDs и будут добавлены.
        
    
    _Дополнение:_ Если возможно, реализовать определение изменений – например, используя `git diff --name-status HEAD~1 HEAD` или сравнение деревьев. Тогда можно в логах плагина отмечать, сколько файлов добавлено/удалено, для отладки. Но для самой Dify это не критично.
    
6. **Потоковая выдача (опционально):** Если репозиторий очень большой, можно реализовать порционную отправку результатов:
    
    - Сначала создать объект `WebSiteInfo` с `status = "processing"` и `completed = 0, total = N` (N – общее число файлов, если известно сразу)[docs.dify.ai](https://docs.dify.ai/en/develop-plugin/dev-guides-and-walkthroughs/datasource-plugin#:~:text=crawl_res%20%3D%20WebSiteInfo%28web_info_list%3D%5B%5D%2C%20status%3D,yield%20self.create_crawl_message%28crawl_res).
        
    - `yield self.create_crawl_message(crawl_res)` – тем самым уведомить Dify, что процесс начался.
        
    - Затем, по мере чтения файлов, накапливать, скажем, по 20-50 документов и выдавать пакетами:
        
        `crawl_res.web_info_list = batch_list crawl_res.completed = k  # сколько файлов уже собрано crawl_res.status = "processing" yield self.create_crawl_message(crawl_res)`
        
        Dify на стороне UI может показать предварительный список найденных “страниц” (в Knowledge UI при использовании Firecrawl пользователь видит список URL-ов, прежде чем нажать Import)[docs.dify.ai](https://docs.dify.ai/versions/3-0-x/en/user-guide/knowledge-base/create-knowledge-and-upload-documents/import-content-data/sync-from-website#:~:text=2).
        
    - В конце установить `status = "completed"`, заполнить оставшиеся данные и yield последний раз.
        
    - Такой механизм сложнее, но повышает отзывчивость. Решение о необходимости оставить на разработчике: для большинства репо можно сразу отдавать финальный результат. Если же репозитории предполагаются огромные, порционная выдача рекомендуется[docs.dify.ai](https://docs.dify.ai/en/develop-plugin/dev-guides-and-walkthroughs/datasource-plugin#:~:text=In%20the%20main%20logic%20code,completed).
        
7. **Обработка ошибок и крайних случаев:** Разработчик должен учесть:
    
    - **Недоступность репозитория:** если SSH-ключ или токен неверны, или репо не существует, плагин должен бросить понятное исключение или вернуть сообщение об ошибке. Dify отобразит ошибку соединения. Лучше отловить ошибки `git` и выдавать `DatasourceError` (если SDK предоставляет такой класс) с сообщением вроде "Authentication failed" или "Repository not found".
        
    - **Пустой репозиторий:** если репо без файлов (такое редко, но возможно) – вернуть пустой список (или возможно хотя бы один элемент-заглушку? Но лучше пустой). Dify может не позволить создать пустую базу знаний, но в целом ничего страшного – пользователь получит сообщение, что нет документов.
        
    - **Большие файлы:** предусмотреть максимальный размер файла для чтения (например, 5-10 MB текста). Если файл больше, можно либо частично его брать, либо пропускать с логом предупреждения. Иначе может съесть много памяти.
        
    - **Несколько веток или монорепозитории:** текущие требования подразумевают одну ветку (по умолчанию main/master). Если нужно, можно сделать параметр “branch” для выбора ветки. По умолчанию плагин может брать default branch.
        
    - **Исключения файлов:** опционально, можно добавить параметр “Exclude patterns” (список строк) и фильтровать файлы. Например, игнорировать `.gitignore`-файлы или папки `/.git/`. Некоторые вещи по умолчанию не нужно индексировать (репозиторий Git содержит `.git` папку, её точно исключить).
        
    - **Совместимость:** убедиться, что после переделки плагин работает в актуальной версии Dify. Возможно, написать несколько юнит-тестов или прогнать его локально, подключив к Dify (самописно или в отладочном режиме с `dify plugin run`).
        
8. **Документация и примеры внутри кода:** Разработчик должен в файле `README.md` плагина отразить, что:
    
    - Плагин теперь синхронизирует **весь репозиторий автоматически**.
        
    - Объяснить, какие типы доступа поддерживаются (SSH, HTTPS, local).
        
    - Указать, как происходит обновление (что нужно нажать Sync, и файлы удалённые из репо пропадут из базы знаний).
        
    - Возможно, привести ограничения (например: “бинарные файлы не индексируются”).
        
    - Упомянуть, что плагин использует `git clone` и потому требует доступ в интернет из контейнера (для Dify Cloud или On-Prem – важно, вдруг нужны прокси настройки).
        
9. **Ссылки на официальные примеры:** В процессе реализации стоит ориентироваться на существующие решения:
    
    - **Firecrawl** – официальный плагин веб-краулера. Он отличается тем, что обращается к внешнему API, но подход к `WebsiteCrawlDatasource` аналогичен. В Dify документации описан принцип его работы и опции (например, обход подстраниц, ограничение глубины)[dify.ai](https://dify.ai/blog/dify-ai-blog-integrated-with-firecrawl#:~:text=Easy%20Setup)[dify.ai](https://dify.ai/blog/dify-ai-blog-integrated-with-firecrawl#:~:text=The%20%27Crawl%20sub,you%20to). Наш плагин не нуждается в таких опциях, но полезно взглянуть, как Firecrawl формирует `WebSiteInfoDetail` на выходе.
        
    - **Jina Reader** – ещё один пример краулера, интегрированного в Dify[docs.dify.ai](https://docs.dify.ai/versions/3-0-x/en/user-guide/knowledge-base/create-knowledge-and-upload-documents/import-content-data/sync-from-website#:~:text=The%20knowledge%20base%20supports%20crawling,it%20into%20the%20knowledge%20base). Возможно, исходники недоступны, но известно, что он тоже возвращает множество страниц.
        
    - **GitHub DataSource (официальный)** – хотя он сделан не на `website_crawl`, а скорее на `online_document`/`tool`, можно посмотреть, как он авторизуется и какие данные извлекает. Судя по описанию на Marketplace, тот плагин умеет брать не только файлы, но и Issues, PR и Wiki[marketplace.dify.ai](https://marketplace.dify.ai/plugins/langgenius/github_datasource#:~:text=Access%20GitHub%20repositories%2C%20issues%2C%20pull,Dify%20with%20comprehensive%20authentication%20support). В нашей задаче мы фокусируемся только на файлах, поэтому наш плагин будет проще.
        
    - **Пример плагина Crawl4AI** – есть сообщество, которое сделало плагин Crawl4AI (аналог Firecrawl). Его код может послужить参考 (ссылка приведена ниже).
        
    - **Официальный репозиторий плагинов Dify** – `langgenius/dify-official-plugins` на GitHub содержит исходники всех официальных плагинов[github.com](https://github.com/langgenius/dify-official-plugins#:~:text=Dify%27s%20models%20and%20tools%20were,users%20to%20explore%20and%20use). Можно найти там папку `datasources` и посмотреть реализацию, например, Notion или Confluence, чтобы понять структуру кода data source плагина (к сожалению, GitHub затрудняет просмотр без авторизации, но при желании можно клонировать репозиторий). С началом Dify v1.0 все встроенные ранее функции вынесены в виде плагинов, и их примеры доступны[github.com](https://github.com/langgenius/dify-official-plugins#:~:text=Dify%27s%20models%20and%20tools%20were,users%20to%20explore%20and%20use).
        
10. **Тестирование:** После реализации разработчику нужно протестировать плагин локально:
    
    - Использовать `dify plugin pack` и `dify plugin run` (CLI) для запуска вне платформы: подставить тестовый репозиторий (сделать маленький репо с парой файлов) и убедиться, что `_get_website_crawl` возвращает правильную структуру (можно эмулировать вызов).
        
    - Затем установить плагин в Dify (например, в локально поднятую Dify Community Edition) и подключить репозиторий, проверить, что документы появились, обновляются, удаляются корректно.
        
    - Тестировать различные режимы доступа: публичный репо, приватный по токену, приватный по SSH (для SSH, возможно, придётся настроить known_hosts или отключить проверку хоста – на усмотрение, но лучше поддержать ключи).
        
    - Проверить, что при ошибочных параметрах Dify выводит сообщение (например, если токен неверный, Dify на этапе “Validate Credentials” выдаст ошибку – Dify обычно при сохранении провайдера пытается сделать тестовый вызов).
        

Сводя воедино, разработчику передаётся задача: **реализовать класс DataSource провайдера типа `website_crawl`, который при запуске клонирует/обновляет заданный Git-репозиторий, и возвращает список документов (файлов) с содержимым.** Каждый файл – отдельный элемент списка (с полями title, content, etc.). Плагин должен учитывать добавление/удаление файлов при повторных запусках (через полный перескан либо дифф), поддерживать аутентификацию для приватных репозиториев, и быть удобным в использовании (минимум ручных шагов от пользователя – только указать репозиторий и при необходимости ключ). В результате пользователь Dify сможет легко интегрировать данные из своего Git-репозитория в базу знаний для RAG, что и является целью.
